# -*- coding: utf-8 -*-
"""Testing.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1yZpneIMYQwuxfPYm3hRebvALoChvWO8L
"""

pip install streamlit

pip install streamlit_option_menu

!pip install GitPython

import streamlit as st
from PIL import Image
import os
import csv
import json
from streamlit_option_menu import option_menu
import subprocess
import plotly.express as px
import pandas as pd
import sqlite3
import requests
import git
import pandas as pd

"""###Data Extraction###"""

# Clone the Github repository
os.system("git clone https://github.com/phonepe/pulse.git")

#path 1 transection agg

import json
import csv

# Define the path to the repository
repo_path = "/content/pulse/data/aggregated/transaction"

# Initialize an empty list to store the data
transaction_data = []

# Loop through each directory in the repository path
for root, dirs, files in os.walk(repo_path):
    for file in files:
        # Check if the file is a JSON file
        if file.endswith(".json"):
            # Open the file and load the JSON data
            with open(os.path.join(root, file), "r") as f:
                json_data = json.load(f)
                # Extract the transaction data and append it to the list
                transaction_data.extend(json_data['data']['transactionData'])

# Write the data to a CSV file
with open("data.csv", "w", newline="") as f:
    writer = csv.writer(f)
    # Write the header row
    writer.writerow(["Transaction Name", "Payment Type", "Count", "Amount"])
    # Write the data rows
    for transaction in transaction_data:
        transaction_name = transaction['name']
        payment_type = transaction['paymentInstruments'][0]['type']
        count = transaction['paymentInstruments'][0]['count']
        amount = transaction['paymentInstruments'][0]['amount']
        writer.writerow([transaction_name, payment_type, count, amount])

df = pd.read_csv("/content/data.csv")
df

import json
import csv
import os

# Define the path to the repository
repo_path = "/content/pulse/data/aggregated/user"

# Initialize an empty list to store the data
tran = []

# Define the keywords to search for in the JSON data
keywords = ["registeredUsers", "appOpens", "usersByDevice"]

# Loop through each directory in the repository path
for root, dirs, files in os.walk(repo_path):
    for file in files:
        # Check if the file is a JSON file
        if file.endswith(".json"):
            # Open the file and load the JSON data
            with open(os.path.join(root, file), "r") as f:
                json_data = json.load(f)
                # Check if the JSON data contains the keywords
                if all(keyword in json_data for keyword in keywords):
                    # Extract the transaction data and append it to the list
                    user_data = {
                        "registeredUsers": json_data["registeredUsers"],
                        "appOpens": json_data["appOpens"]
                    }
                    for device_data in json_data["usersByDevice"]:
                        user_data["brand"] = device_data["brand"]
                        user_data["count"] = device_data["count"]
                        user_data["percentage"] = device_data["percentage"]
                        tran.append(user_data)

# Write the data to a CSV file
with open("agg_data_user.csv", "w", newline="") as f:
    writer = csv.writer(f)
    # Write the header row
    writer.writerow(["Registered Users", "Appopens", "Brand", "Count", "Percentage"])
    # Iterate over the transaction data
    for user in tran:
        # Write a row for each user and device combination
        writer.writerow([
            user["registeredUsers"],
            user["appOpens"],
            user["brand"],
            user["count"],
            user["percentage"]
        ])

df = pd.read_csv("/content/agg_data_user.csv")
df

import csv
import json
import os

# Define the path to the repository
repo_path = "/content/pulse/data/map/transaction"

# Initialize an empty list to store the data
transaction_data = []

# Loop through each directory in the repository path
for root, dirs, files in os.walk(repo_path):
    for file in files:
        # Check if the file is a JSON file
        if file.endswith(".json"):
            # Open the file and load the JSON data
            with open(os.path.join(root, file), "r") as f:
                json_data = json.load(f)
                # Extract the transaction data and append it to the list
                if 'data' in json_data and 'hoverDataList' in json_data['data']:
                    transaction_data.extend(json_data['data']['hoverDataList'])

# Write the data to a CSV file
with open("Map_transection.csv", "w", newline="") as f:
    writer = csv.writer(f)
    # Write the header row
    writer.writerow(["State", "Type", "Count", "Amount"])
    # Iterate over the transaction data
    for item in transaction_data:
        location = item['name']
        metric_type = item['metric'][0]['type']
        count = item['metric'][0]['count']
        amount = item['metric'][0]['amount']
        writer.writerow([location, metric_type, count, amount])

df = pd.read_csv("/content/Map_transection.csv")
df

import csv
import json
import os

# Define the path to the repository
repo_path = "/content/pulse/data/map/user"

# Initialize an empty list to store the data
transaction_data = []

# Loop through each directory in the repository path
for root, dirs, files in os.walk(repo_path):
    for file in files:
        # Check if the file is a JSON file
        if file.endswith(".json"):
            # Open the file and load the JSON data
            with open(os.path.join(root, file), "r") as f:
                json_data = json.load(f)
                # Extract the transaction data and append it to the list
                for state, data in json_data['data']['hoverData'].items():
                    transaction_data.append({
                        'state': state,
                        'registeredUsers': data['registeredUsers'],
                        'appOpens': data['appOpens']
                    })

# Write the data to a CSV file
with open("Map_user.csv", "w", newline="") as f:
    writer = csv.writer(f)
    # Write the header row
    writer.writerow(["State", "RegisteredUser", "Appopens" ])
    # Iterate over the transaction data
    for item in transaction_data:
        state = item['state']
        registered_users = item['registeredUsers']
        app_opens = item['appOpens']
        writer.writerow([state, registered_users, app_opens])

df = pd.read_csv("/content/Map_user.csv")
df

import csv
import json
import os

# Define the path to the repository
repo_path = "/content/pulse/data/top/transaction"

# Initialize an empty list to store the data
transaction_data = []

# Loop through each directory in the repository path
for root, dirs, files in os.walk(repo_path):
    for file in files:
        # Check if the file is a JSON file
        if file.endswith(".json"):
            # Open the file and load the JSON data
            with open(os.path.join(root, file), "r") as f:
                json_data = json.load(f)
                # Extract the transaction data and append it to the list
                
                transaction_data.extend(json_data['data']['districts'])

# Write the data to a CSV file
with open("top_trans.csv", "w", newline="") as f:
    writer = csv.writer(f)
    
    # Write the header row
    writer.writerow(["State","Type", "Count", "Amount"])
    
    # Iterate over the data and write to the CSV file
    for item in transaction_data:
        location = item['entityName']
        metric_type = item['metric']['type']
        count = item['metric']['count']
        amount = item['metric']['amount']
        writer.writerow([location, metric_type, count, amount])

df = pd.read_csv("/content/top_trans.csv")
df

import csv
import json
import os

# Define the path to the repository
repo_path = "/content/pulse/data/top/user"

# Initialize an empty list to store the data
transaction_data = []

# Loop through each directory in the repository path
for root, dirs, files in os.walk(repo_path):
    for file in files:
        # Check if the file is a JSON file
        if file.endswith(".json"):
            # Open the file and load the JSON data
            with open(os.path.join(root, file), "r") as f:
                json_data = json.load(f)
                # Extract the transaction data and append it to the list
                
                transaction_data.extend(json_data['data']['districts'])

# Write the data to a CSV file
with open("top_user.csv", "w", newline="") as f:
    writer = csv.writer(f)
    
    # Write the header row
    writer.writerow(["State", "Registered_Users"])
    
    # Iterate over the data and write to the CSV file
    for item in transaction_data:
      location = item['name']
      registeredusers = item["registeredUsers"]
      writer.writerow([location, registeredusers])

df = pd.read_csv("/content/top_user.csv")
df

"""###Joining CSV files ###"""

import pandas as pd

# Define the CSV file names and the separator character
file_names =['/content/Map_transection.csv', "/content/Map_user.csv", "/content/agg_data_user.csv", "/content/data.csv", "/content/top_trans.csv", "/content/top_user.csv"]

separator = ","

# Load the CSV files into pandas dataframes
dfs = [pd.read_csv(f, sep=separator) for f in file_names]

# Merge the dataframes on a common column
merged_df = pd.concat(dfs, axis=0, ignore_index=True)

# Save the merged dataframe as a new CSV file
merged_df.to_csv("merged_file.csv", index=False)

df = pd.read_csv("/content/merged_file.csv")
df

df.loc[20000]

"""###Cleaning & Preprocessing###"""

# check for missing values
print(df.isna().sum())

# impute missing values
df.fillna(0, inplace=True)

# Drop any rows with missing values
df = df.dropna()

df

df.shape

df.describe()

# Remove outliers
df = df[(df['column'] > lower_bound) & (df['column'] < upper_bound)]

# Filter data
df = df[df['column'] == value]

# Aggregate data
df_grouped = df.groupby('category').sum()

# Remove outliers
q1 = data['Revenue'].quantile(0.25)
q3 = data['Revenue'].quantile(0.75)
iqr = q3 - q1
upper = q3 + 1.5 * iqr
lower = q1 - 1.5 * iqr
data = data[(data['Revenue'] >= lower) & (data['Revenue'] <= upper)]

# Create new features
data['Profit'] = data['Revenue'] - data['Expenses']
data['Year'] = pd.DatetimeIndex(data['Date']).year
data['Month'] = pd.DatetimeIndex(data['Date']).month

# Normalize the data
data['Revenue'] = (data['Revenue'] - data['Revenue'].mean()) / data['Revenue'].std()
data['Expenses'] = (data['Expenses'] - data['Expenses'].mean()) / data['Expenses'].std()
data['Profit'] = (data['Profit'] - data['Profit'].mean()) / data['Profit'].std()

# Split the data into training and testing datasets
train_data = data[data['Year'] < 2022]
test_data = data[data['Year'] >= 2022]

# Aggregate the data by country and month
grouped_data = data.groupby(['Country', 'Month']).agg({'Revenue': 'sum', 'Expenses': 'sum', 'Profit': 'mean'})

#visualisation

"""###CONNECTION WITH 
***SQL SERVER***
"""

pip install ipython-sql

import MySQLdb

# Commented out IPython magic to ensure Python compatibility.
# %%sql mysql://root:foobaOomkar@123@172.20.101.81/pidata

import os

conn = ps.connect(
    host=os.environ["db_host"],
    port=os.environ["db_port"],
    dbname=os.environ["db_name"],
    user=os.environ["root"],
    password=os.environ["Oomkar@123"])

df = pd.read_sql_query("select * from <table>", con=conn)

import mysql.connector

cnx = mysql.connector.connect(user='root', password='Oomkar@123',
                              host='127.0.0.1',
                              database='phonepe')
cnx.close()

import mysql.connector
from mysql.connector import Error
import pymysql


db = mysql.connector.connect(
   host="localhost",
   user="mamp",
   passwd="Oomkar@123"
)

print(db)













cursor.close()
cnx.close()

import streamlit as st
import pandas as pd
import plotly.express as px
import plotly.graph_objs as go
import time

# Set the title and layout of the dashboard
st.set_page_config(page_title='Live Geo Dashboard', layout='wide')

# Load the data from the MySQL database
def load_data():
    # Establish a connection to the MySQL database
    connection = mysql.connector.connect(
        host='localhost',
        user='your_username',
        password='your_password',
        database='your_database'
    )
    
    # Query the data from the database
    query = 'SELECT country, latitude, longitude, revenue FROM my_table'
    data = pd.read_sql(query, con=connection)
    
    # Close the database connection
    connection.close()
    
    return data

# Define the function to update the map
def update_map():
    # Load the data
    data = load_data()
    
    # Group the data by country and aggregate the revenue
    data_agg = data.groupby(['country', 'latitude', 'longitude']).agg({'revenue': 'sum'}).reset_index()
    
    # Create the map
    fig = px.scatter_mapbox(data_agg, lat='latitude', lon='longitude', size='revenue', color='country', zoom=2,
                            hover_name='country', hover_data={'revenue': ':.2f'})
    fig.update_layout(mapbox_style='carto-positron')
    fig.update_layout(margin={'l':0, 't':0, 'b':0, 'r':0})
    
    return fig

# Define the Streamlit app
def app():
    # Set the title and subtitle
    st.title('Live Geo Dashboard')
    st.subheader('Real-time visualization of revenue by country')
    
    # Create the initial map
    fig = update_map()
    
    # Display the map
    st.plotly_chart(fig, use_container_width=True)
    
    # Update the map every 5 seconds
    while True:
        fig = update_map()
        st.plotly_chart(fig, use_container_width=True)
        time.sleep(5)

if __name__ == '__main__':
    app()

import mysql.connector
import pandas as pd

# Function to load data from MySQL database
def load_data():
    # Establish a connection to the MySQL database
    connection = mysql.connector.connect(
        host='localhost',
        user='your_username',
        password='your_password',
        database='your_database'
    )
    
    # Query the data from the database
    query = 'SELECT * FROM your_table'
    data = pd.read_sql(query, con=connection)
    
    # Close the database connection
    connection.close()
    
    return data

import streamlit as st
import pandas as pd
import mysql.connector
import plotly.express as px

# Function to load data from MySQL database
def load_data():
    # Establish a connection to the MySQL database
    connection = mysql.connector.connect(
        host='localhost',
        user='your_username',
        password='your_password',
        database='your_database'
    )
    
    # Query the data from the database
    query = 'SELECT * FROM your_table'
    data = pd.read_sql(query, con=connection)
    
    # Close the database connection
    connection.close()
    
    return data

# Load data from MySQL database
data = load_data()

# Define dropdown options
options = {
    'Total number of transactions': 'count',
    'Total transaction value': 'sum',
    'Average transaction value': 'mean',
    'Transaction volume by payment method': 'groupby_payment_method.count',
    'Transaction volume by category': 'groupby_category.count',
    'Top 5 categories by transaction volume': 'groupby_category.count().nlargest(5)',
    'Top 5 payment methods by transaction volume': 'groupby_payment_method.count().nlargest(5)',
    'Transaction volume by day of the week': 'groupby_dayofweek.count',
    'Transaction volume by hour of the day': 'groupby_hourofday.count',
    'Average transaction value by category': 'groupby_category.mean'
}

# Define function to create visualizations based on user selection
def create_visualization(selection):
    if selection == 'Total number of transactions':
        st.write('Total number of transactions:', data['Transaction ID'].count())
    elif selection == 'Total transaction value':
        st.write('Total transaction value:', data['Transaction Amount (INR)'].sum())
    elif selection == 'Average transaction value':
        st.write('Average transaction value:', data['Transaction Amount (INR)'].mean())
    elif selection == 'Transaction volume by payment method':
        fig = px.histogram(data, x='Payment Method', color='Payment Method')
        st.plotly_chart(fig)
    elif selection == 'Transaction volume by category':
        fig = px.histogram(data, x='Category', color='Category')
        st.plotly_chart(fig)
    elif selection == 'Top 5 categories by transaction volume':
        top_categories = eval('data.' + options[selection])
        fig = px.bar(top_categories, x=top_categories.index, y='Transaction ID')
        st.plotly_chart(fig)
    elif selection == 'Top 5 payment methods by transaction volume':
        top_payment_methods = eval('data.' + options[selection])
        fig = px.bar(top_payment_methods, x=top_payment_methods.index, y='Transaction ID')
        st.plotly_chart(fig)
    elif selection == 'Transaction volume by day of the week':
        fig = px.histogram(data, x='Day of the week', color='Day of the week')
        st.plotly_chart(fig)
    elif selection == 'Transaction volume by hour of the day':
        fig = px.histogram(data, x='Hour of the day', color='Hour of the day')
        st.plotly_chart(fig)
    elif selection == 'Average transaction value by category':
        avg_by_category = eval('data.' + options[selection])
        fig = px.bar(avg_by_category, x=avg_by_category.index, y='Transaction Amount (INR)')
        st.plotly_chart(fig)

# Create Streamlit app
st.title('Phonepe Pulse Dashboard')
st.sidebar.title('Select an option')
selection = st.sidebar.selectbox('', list(options.keys()))

# Create data groups for dropdown options
data



















































